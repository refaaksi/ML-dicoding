{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-dicoding-Submission-Refa Rupaksi.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhQTB4L6PTkOQF9ZW7Viok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/refaaksi/ML-dicoding/blob/main/ML_dicoding_Submission_Refa_Rupaksi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyDwXf2894pc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Input,\n",
        "    MaxPooling2D,\n",
        "    GlobalAveragePooling2D, \n",
        "    GlobalMaxPooling2D,\n",
        "    Activation,\n",
        "    Dropout\n",
        ")\n",
        "from tensorflow.keras import Model\n",
        "import zipfile, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqj_sruZkhRB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhRk54sSckqR"
      },
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "print('Available GPUs: ', len(tf.config.list_logical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0owTCw2Q9LT2"
      },
      "source": [
        "!wget command: --no-check-certificate \\\n",
        "  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip \\\n",
        "  -O /tmp/rockpaperscissors.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPomO0mb-RLW"
      },
      "source": [
        "# melakukan ekstraksi pada file zip\n",
        "local_zip = '/tmp/rockpaperscissors.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/rockpaperscissors/rps-cv-images/'\n",
        "os.listdir(base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goSukmYCqFVo"
      },
      "source": [
        "os.chdir(base_dir)\n",
        "with open('README_rpc-cv-images.txt', 'r') as f:\n",
        "  r = f.read(-1)\n",
        "\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poxsVtZ2rdfc"
      },
      "source": [
        "BATCH_SIZE=32\n",
        "\n",
        "def normalize_img(image,label):\n",
        "  return tf.cast(image, tf.float32)/255, label\n",
        "\n",
        "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    labels = 'inferred',\n",
        "    label_mode = \"categorical\",\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode = 'rgb',\n",
        "    image_size = (300,200),\n",
        "    seed = 0,\n",
        "    shuffle = True,\n",
        "    validation_split = 0.4,\n",
        "    subset = \"training\")\n",
        "\n",
        "ds_val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    labels = 'inferred',\n",
        "    label_mode = \"categorical\",\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode = 'rgb',\n",
        "    image_size = (300,200),\n",
        "    seed = 0,\n",
        "    shuffle = True,\n",
        "    validation_split = 0.4,\n",
        "    subset = \"validation\")\n",
        "\n",
        "ds_train = ds_train.map(normalize_img)\n",
        "ds_val = ds_val.map(normalize_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEhq-aihhCZg"
      },
      "source": [
        "data_augmentation = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVponJrIhxmf"
      },
      "source": [
        "def conv_block(units, activation='relu', block=1, layer=1):\n",
        "\n",
        "    def layer_wrapper(inp):\n",
        "        x = Conv2D(units, (3, 3), name='block{}_conv{}'.format(block, layer))(inp)\n",
        "        # x = BatchNormalization(name='block{}_bn{}'.format(block, layer))(x)\n",
        "        x = Activation(activation, name='block{}_act{}'.format(block, layer))(x)\n",
        "        return x\n",
        "\n",
        "    return layer_wrapper\n",
        "\n",
        "def dense_block(units, dropout=0.2, activation='relu', name='fc1'):\n",
        "\n",
        "    def layer_wrapper(inp):\n",
        "        x = Dense(units, name=name)(inp)\n",
        "        #x = BatchNormalization(name='{}_bn'.format(name))(x)\n",
        "        x = Activation(activation, name='{}_act'.format(name))(x)\n",
        "        x = Dropout(dropout, name='{}_dropout'.format(name))(x)\n",
        "        return x\n",
        "\n",
        "    return layer_wrapper\n",
        "\n",
        "def make_model(input_shape, classes, dropout=0.3, activation='relu'):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    X_input = data_augmentation(inputs)\n",
        "\n",
        "    x = conv_block(32, activation=activation, block=1, layer=1)(X_input)\n",
        "    x = MaxPooling2D((2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = conv_block(64, activation=activation, block=2, layer=1)(x)\n",
        "    x = MaxPooling2D((2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = conv_block(128, activation=activation, block=3, layer=1)(x)\n",
        "    x = MaxPooling2D((2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # # Block 4\n",
        "    x = conv_block(256, activation=activation, block=4, layer=1)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    x = conv_block(512, activation=activation, block=5, layer=1)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    # Flatten\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = dense_block(512, dropout=dropout, activation=activation, name='fc1')(x)\n",
        "    \n",
        "    # Classification block    \n",
        "    outputs = Dense(classes, activation='softmax', name='predictions')(x)\n",
        "\n",
        "    return Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anNvUTUTlOmW"
      },
      "source": [
        "model = make_model(input_shape = (300, 200, 3), classes = 3)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1OUepDkm0m6"
      },
      "source": [
        "history = model.fit(\n",
        "      ds_train,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=25,\n",
        "      validation_data=ds_val,\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0BzCSgAEF6K"
      },
      "source": [
        "#eval = model.evaluate(ds_val, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LFnLvq94dp8"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#loss train & validation\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPCMDVzm4kEv"
      },
      "source": [
        "#accuracy train & validation\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Plot')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
